- Different implementations of BLAS - taken into account
- Cost of BLAS functions initially done by going through the Fortran code
- Wondered about the cost of rand(), sqrt... - count them as 1 (happen rarely)
- Wondered if we should count comparison as an op - no (happens rarely)
- Making BLAS single threaded
- OpenBLAS first! - if we have time, we can do a little analysis on different BLAS impls and their performance
- For benchmarking
	- used seconds so we can run code in all languages
	- same number of iterations
	- same init method - random
	- one machine, but different than the one we ran our code on regularly (Viktor's computer)
	- R is 3
	- ran 3 times and averaged
	- ran impls up to the sizes it could handle
- note the R value for each plot

- Blas performance:
	- MMM performance drastically decreases when the matrix size is such that the common dimension is small. The MMM is in fact closer to a matrix vector
	  multiplication. In tests when multiplying matrices (400 x 3) (3 x 400) the performance drops from 12 f/c to 2 f/c. This is expected because the computation
	  now basically have a constant opertational intensity. 


- Optimizations:
*Basic optimizations:
	- everything was put into a single file
	- functions were marked as inline
	- removed struct, now just using double*
	- removed function calls for allocation and deallocation
	- calculation of the norm of V is now done only once
	- matrix needed to calculate error (approx) is allocated only once
	- removed call for calculating the norm - now done in place
	- in the error function, two loops over the approx matrix were 	replaced with one + scalar replacement added and number of 	accesses to memory reduced by 1/3 per function call
	- error function now takes 1 / norm of V as a parameter so a div 	was replaced with a mul
	- dimensions of all matrices calculated only once and reused
	- double loops over matrices replaced with a single loop going 	sequentially over all elements (index calc simplified)
	- 1 / RAND_MAX was precalculated so all divisions during init are 	now multiplications

*Alg_opt_1:
	- We tried reusing matrix W in the computation of Hn+1 computing WtW and WtV in the same loop.
	  Matrix W and V are transposed to improve cache locality. The obtained performance with 
	  (m,n,r) = (200, 200, 5) is 0.68.

*Optimizations 4
	- Loop unrolling for norm calculation to optimize error function
		- No improve (as expected since I was opitimizing lower terms in the cost)


*Optimization 5:
	- still to be finished, the goal was to apply an optimization similar to
	  opt 6 on alg opt 2. We believe it might not be applicable. 
	  


*Optimization 6:
	- blocking for cache and using BLAS for inner multiplitcation
	- some transpose can be avoided but we tested and the transpose is	
	  time is negligible
	- performance highly depends on the rank (the higher the better) to	
	  use blas the most
	

*Optimization 7:
	- the basic optimizations made in opt1 were transfered on baseline2 
	  as well. Loop unrolling for the norm is also used. 


